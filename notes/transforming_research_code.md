# Moving beyond 'research code'
*You just inherited a codebase that does X. Minimal comments, little documentation, and written for someone's publication/thesis/side-project. You know it'll do what it was designed for, but not entirely sure it won't break when you feed it slightly different inputs. You realise you need to re-factor the code and generalise it. Here I detail the strategies that helped me re-factor code bases (after complaining a bit about the state of 'research code' in science)*

## 'Research code' is a euphemism for use-and-throw
We often inherit a piece of code from a colleague that 'does this thing'. This 'piece of code' can vary from a single file with tens of lines to a multi-file codebase. If you are lucky enough, it actually runs on your system with a just few path changes. If you are even luckier, it actually does the thing its supposed to do. The truth is however, it most likely will not do the thing *you* want it to do, and *you* will have a tough time understanding how it all makes sense. You might also be left feeling like you're the dumb one who can't figure out what's going wrong. We all know the classic line "it's research code, it might be a bit rough on the edges". 

To clarify, this is not me bitching and raging on all the people who have ever shared code with me (I know I have shared my fair share of 'research code' with some of you reading this!). Yes, not all code needs to be production-perfect or written to industry standard, and yet I do see lots of scope for improval. This is me openly describing the situation as it stands, with the hope that some of you will sympathise! Fair warning - there is nothing mind-blowingly new in this blog post. In all likelihood, reading a few peer-reviewed papers on the topic, and some common sense will get you a long way. But then again, common sense is often a matter of hindsight, and I also do go into detail on some points that peer-reviewed papers may find too 'basic'.

## Research code is use-and-throw, but is weirdly 'expensive'

The fact is: research code is often written to get one very specialised task done for one very specialised scenario. Its use-and-throw nature inherently means another person may not be able to run it easily or manipulate it to their own needs. A grad student writes a couple of functions for their first manuscript, once its accepted and uploaded somewhere, onto the next project. A postdoc inherits a piece of code from a previous member, they begin going through the codebase. After pulling their hair for a couple of weeks, they realise it'll be easier starting from scratch and re-coding it all. 

What bothers me about the 'use-and-throw' attitude to research code is that it is not *cheap* by any metric you use to measure it. Time, effort, innovation - all of it often goes into writing research code, whether it be a statistical analysis or a method to identify your study animal from image or audio data.

## From 'use-and-throw' to 'use-and-share'
Till now I've dealt with various types of 'research code', most of it inherited from my past-self (the guy who wrote this code three months ago), and some of it from colleagues, mentors or just random code found on the internet.I have in general been interested in the idea of making the code we write more rigorous, acessible and easy to share. As suggested by previous publications (See list of papers below), it is completely true that using basic methods like version control, unit-testing and documenting code, any codebase becomes easier to handle. 

While the methods detailed here are not new, I still do think this post may be interested for those of you who have a nice piece of code that was designed for task x.1.2 (e.g. finding the duration of a bat call for the first chapter of your PhD thesis), but is very likely generalisable to task X (measuring sound duration). I'm hoping the steps outlined below will allow you to convert the code from 'use-and-throw' to 'use-and-share'. While I suggest ways to transform the code, the same steps can be used to avoid the issues described in the first place.

### Steps

1. **Document, document, document**: Any piece of code is very likely to have a few main functions that do the heavy lifting. Understand what the expected inputs and outputs are, and write them down. If it means sitting with the original person who wrote the code, then try to make the time for it as soon as you get the code. Remember, the details of how code works is often forgotten over the long-term. So, even if you thought you understood how it all worked when the author explained it all to you, this knowledge will vapourise when you need it a few months down the line. Examples of things to document are: compulsory and optional inputs, if optional what are the default values? Input object types (integer, float, complex number, array) and lengths/dimensions (number of entries or row x column dimensions). Not sure how your documentation should look like? Most computer languages typically have a few common conventions used by established packages. I for instance use the Numpy convention to write docstrings in Python. 

	Documenting need not only mean writing down the computational details of the code. Here I use documenting to mean writing down the *why* and *how* of the code. Imagine there are four modules that need to be run in a specific order - it's important to write this information down too!

1. **Separate the 'doing' from the 'assessing'**: You're coding up a new algorithm to do *insertamazingtask*. As you're writing the function to implement the task, you also include portions that test the algorithm's accuracy/performance with the ground truth data. This is understandable, and is likely what happens if you're in a rush to reach the looming deadline. The only issue is this - you lose out on the generality of your code, and in a major way. Separating the code that *does* the task from the code that *quantifies* the performance of your code takes you a long way in increasing usability. You'll thank yourself later when you reuse the code, as will the next person you share the code with! 

1. **Testing allows you to build without breaking**: Another classic dialogue "it worked for me, I'm  sure you can fiddle around a bit for your case". It's awesome when you see the code work for your case, but the parameters needs a bit more fiddling...hmmm. So you start fiddling around, renaming variables, removing some (what you think are) unnecessary parameters, and a few minutes into it nothing seems to work anymore. A lot of things have changed, and there's no coming back it seems.

	Things do break often, and that's not bad - as long as you can trace why and when. Start by writing unit-tests for all of the important functions in your code base. Imagine the ```reshape_weather_data``` takes in three variables ```temperature, weather, operator```. Somewhere along the way, you go on to remove ```operator``` input requirement. ```reshape_weather_data``` weather data still works as a function - but there may be some downstream functions that rely on the ```operator``` variable being included in their input. You can of course figure it all out by some good old debugging, but this is pretty intensive. Instead imagine this. You write unit-tests for ```reshape_weather_data```. When you next make the alteration to the inputs and run tests, you immediately get an error message saying the number of inputs are incorrect. Now you're alerted, and you *know* where the problem was. Much easier no? This speed-up in change-detection only increases with every change you make to the codebase. 

1. **Remove hard-coded paths and values**: Unless there's a very very very good reason for values to be hard-coded (a value designated manually in the code), don't do it. Perhaps there's some kind of situation where you know a value is a physical constant that'll never need to be changed, okay. Otherwise, just don't. The same applies for file and object paths. Avoid hard-coded file path - they're an obvious way to make your code fail when you run it on another system. Any kind of file-path/string should raise eyebrows, and you must try to eliminate them.

1. **Remove file-writing side effects**: You know that thing where you write a function ```process_raw_data``` that takes in raw data, cleans it up and writes it into a csv file, and returns nothing? That's a classic side effect in a function. It seems pretty harmless right, you just wrote a file - and this means you can load it in later, or actually open it in another software (OpenOffice or Excel for example)! Well, the fact is it makes your life later on a bit more complicated. Imagine you now run ```generate_density_map```, which reads the csv file generated by ```process_raw_data```, but you forgot to run ```process_raw_data``` before. You're unintentionally feeding in an older pre-existing, hard-to-track input - with potential for serious error.

	Preferably get rid of any file-writing patterns, unless absolutely necessary, and also try to output the to-be-written data as an object (dataframe, dictionary, etc.). Having well-defined input and output objects allows you and another person to clearly predict and check what goes in and comes out. Of course, if the function outputs can't be stored in memory due to size or time and must be written to disk - then perhaps implement a proper scheme to avoid confusion in execution order.

1. **Alter names to be meaningful**: Anyone who's begun to work with a piece of code with variables named 'va', 'fm', or the classic 'x' will at some point start to worry about the time when this code needs to be refactored. Short variable names are a historical and cultural artifact of the first computing systems which had small screens and memory - and thus the cryptic names. We are now in the age of comfortably sized monitors, even ultra-wide ones in fact. There are no real reasons to continue this historically outdated practice. Yes, I too learnt to name my variables 'xx1', 'xx2, etc. just like many of you, but it doesn't make sense once you stop and start to think about the historical roots! Instead of 'va' isn't 'vertical_alignment' better, doesn't 'fundamental_matrix' make so much more sense than 'fm'? Opaque variable names are deep sinkholes of time and debugging effort - save yourself a lot of frustration and err on the side of descriptive names!

1. **Include as many references as you go**: Especially if you're working with code implementing a new method, you will probably rely on concepts, ideas and methods from published literature, or even your *own* thesis/paper/preprint. As happens in science, it may be no surprise to realise that what some people call a 'central body', others will call 'common body', and then someone will also come up with the more general term 'circular body'. Now combine working with short variable names where the variable 'c_body' appears in different functions, and likely has slightly different meanings in each situation. Ideally, of course, the 'c_body' could have been name something more informative. Given that we still have the 'c_body' variable spread over more multiple functions - what could have helped to make sense -- references! A short reference as part of the function documentation saying 'c_body in this function refers to the central body in ABC et al. 2022' goes a long way in explaining to your future self, or the next person who comes along.

	References need not only mean the classic 'FirstAuthor et al. 2022', but it may even include detailed descriptions, equation numbers, and pointers to errata. The more detail the better remember. Much like cryptic variable names - a function implementing a method whose source is uncited is tricky to verify!

## Overview
To summarise, 'research code' is often a proof-of-principle thing. Despite it being proof-of-principle, I argue that it shouldn't imply 'hard-to-generalise' or 'difficult-to-understand'! All 'research code' is admittedly written under a time-constraint, projects need to be submitted by the end of the semester, PhD, postdoc and project funding timelines. 

Hopefully the steps above will help you if you're dealing with some form of research code handed down to you. In the best case, this post may also form a set of guidelines when you're writing your next piece of code to do *InsertCoolTaskHere*. Perfection is not the goal, and every tiny good practice implemented adds to code quality. Keep calm, and code on. 

### Relevant reading
* Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, et al. (2014) Best Practices for Scientific Computing. PLoS Biol 12(1): e1001745. https://doi.org/10.1371/journal.pbio.1001745
* Poisot, T. (2015). Best publishing practices to improve user confidence in scientific software. Ideas in Ecology and Evolution, 8(1).https://ojs.library.queensu.ca/index.php/IEE/article/download/5644/5463
